{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from -r requirement.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from -r requirement.txt (line 2)) (0.3.9)\n",
      "Requirement already satisfied: langchain_core in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from -r requirement.txt (line 3)) (0.3.21)\n",
      "Requirement already satisfied: langchain_community in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from -r requirement.txt (line 4)) (0.3.9)\n",
      "Requirement already satisfied: langchain-ollama in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from -r requirement.txt (line 5)) (0.2.1)\n",
      "Requirement already satisfied: langchain_text_splitters in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from -r requirement.txt (line 6)) (0.3.2)\n",
      "Requirement already satisfied: langchain-huggingface in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from -r requirement.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from -r requirement.txt (line 8)) (1.9.0.post1)\n",
      "Requirement already satisfied: chromadb in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from -r requirement.txt (line 10)) (0.5.23)\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from -r requirement.txt (line 11)) (3.3.1)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from -r requirement.txt (line 12)) (0.8.0)\n",
      "Requirement already satisfied: pypdf in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from -r requirement.txt (line 13)) (5.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (3.11.10)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (2.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain_core->-r requirement.txt (line 3)) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain_core->-r requirement.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain_core->-r requirement.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain_community->-r requirement.txt (line 4)) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain_community->-r requirement.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain_community->-r requirement.txt (line 4)) (2.6.1)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain-ollama->-r requirement.txt (line 5)) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain-huggingface->-r requirement.txt (line 7)) (0.26.5)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain-huggingface->-r requirement.txt (line 7)) (0.20.3)\n",
      "Requirement already satisfied: transformers>=4.39.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langchain-huggingface->-r requirement.txt (line 7)) (4.46.3)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (0.115.6)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 10)) (0.32.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (3.7.4)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (1.20.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (1.28.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (1.68.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (4.2.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (31.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (3.10.12)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (0.27.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from chromadb->-r requirement.txt (line 10)) (13.9.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from sentence-transformers->-r requirement.txt (line 11)) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from sentence-transformers->-r requirement.txt (line 11)) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from sentence-transformers->-r requirement.txt (line 11)) (1.14.1)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from sentence-transformers->-r requirement.txt (line 11)) (11.0.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from tiktoken->-r requirement.txt (line 12)) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (1.18.3)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from build>=1.0.3->chromadb->-r requirement.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community->-r requirement.txt (line 4)) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community->-r requirement.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb->-r requirement.txt (line 10)) (0.41.3)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb->-r requirement.txt (line 10)) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb->-r requirement.txt (line 10)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb->-r requirement.txt (line 10)) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb->-r requirement.txt (line 10)) (3.10)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb->-r requirement.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb->-r requirement.txt (line 10)) (0.14.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface->-r requirement.txt (line 7)) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface->-r requirement.txt (line 7)) (2024.10.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain_core->-r requirement.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 10)) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 10)) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 10)) (2.36.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 10)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 10)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 10)) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 10)) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 10)) (0.9)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirement.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 10)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 10)) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 10)) (5.29.1)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 10)) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirement.txt (line 10)) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirement.txt (line 10)) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirement.txt (line 10)) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.2 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirement.txt (line 10)) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.2 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirement.txt (line 10)) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b2 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirement.txt (line 10)) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.49b2 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirement.txt (line 10)) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirement.txt (line 10)) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.49b2 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirement.txt (line 10)) (0.49b2)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirement.txt (line 10)) (1.17.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirement.txt (line 10)) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb->-r requirement.txt (line 10)) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb->-r requirement.txt (line 10)) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirement.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirement.txt (line 2)) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from requests<3,>=2->langchain->-r requirement.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from rich>=10.11.0->chromadb->-r requirement.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from rich>=10.11.0->chromadb->-r requirement.txt (line 10)) (2.18.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirement.txt (line 11)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirement.txt (line 11)) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from transformers>=4.39.0->langchain-huggingface->-r requirement.txt (line 7)) (0.4.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from typer>=0.9.0->chromadb->-r requirement.txt (line 10)) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from typer>=0.9.0->chromadb->-r requirement.txt (line 10)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 10)) (0.6.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 10)) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 10)) (14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from scikit-learn->sentence-transformers->-r requirement.txt (line 11)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from scikit-learn->sentence-transformers->-r requirement.txt (line 11)) (3.5.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirement.txt (line 10)) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirement.txt (line 10)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirement.txt (line 10)) (4.9)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirement.txt (line 10)) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirement.txt (line 10)) (0.1.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community->-r requirement.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 10)) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers->-r requirement.txt (line 11)) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirement.txt (line 10)) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the PDF\n",
    "loader = PyPDFLoader(\"docs/rag_chunking_example.pdf\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 0}, page_content='RAG Chunking Example Document\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 0}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 0}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 0}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 0}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nPage 1'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 1}, page_content='RAG Chunking Example Document\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 1}, page_content='accuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 1}, page_content='semantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 1}, page_content='various chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nPage 2'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 2}, page_content='RAG Chunking Example Document\\nfor learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 2}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 2}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 2}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.\\nPage 3'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 3}, page_content='RAG Chunking Example Document\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 3}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 3}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 3}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 3}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\nPage 4'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 4}, page_content='RAG Chunking Example Document\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 4}, page_content='manageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 4}, page_content='accuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 4}, page_content='semantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 4}, page_content='various chunking techniques, including overlapping windows, recursive splitting, and semantic\\nPage 5'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 5}, page_content='RAG Chunking Example Document\\nsegmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 5}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 5}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 5}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nPage 6'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 6}, page_content='RAG Chunking Example Document\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 6}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 6}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 6}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 6}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nPage 7'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 7}, page_content='RAG Chunking Example Document\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 7}, page_content='accuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 7}, page_content='semantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 7}, page_content='various chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nPage 8'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 8}, page_content='RAG Chunking Example Document\\nfor learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 8}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 8}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 8}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.\\nPage 9'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 9}, page_content='RAG Chunking Example Document\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 9}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 9}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 9}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 9}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\nPage 10'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 10}, page_content='RAG Chunking Example Document\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 10}, page_content='manageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 10}, page_content='accuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 10}, page_content='semantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 10}, page_content='various chunking techniques, including overlapping windows, recursive splitting, and semantic\\nPage 11'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 11}, page_content='RAG Chunking Example Document\\nsegmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 11}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 11}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 11}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nPage 12'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 12}, page_content='RAG Chunking Example Document\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 12}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 12}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 12}, page_content='for learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 12}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nPage 13'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 13}, page_content='RAG Chunking Example Document\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for\\nsemantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 13}, page_content='accuracy and processing speed. This document is structured to facilitate experimentation with\\nvarious chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nmanageable chunks. Each chunk represents a coherent section of information that can be used for'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 13}, page_content='semantic search and retrieval tasks. By creating this document, we aim to provide a realistic dataset\\nfor learning and practicing chunking strategies.\\nChunking is a crucial step in many AI-driven pipelines. Effective chunking ensures that the text is\\ndivided into sections that are neither too long nor too short. This helps optimize both retrieval\\naccuracy and processing speed. This document is structured to facilitate experimentation with'), Document(metadata={'source': 'docs/rag_chunking_example.pdf', 'page': 13}, page_content='various chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nPage 14')]\n"
     ]
    }
   ],
   "source": [
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/basic-rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Initialize embeddings model (using SentenceTransformers)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create Chroma database\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM  # Correct import\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm = OllamaLLM(model=\"llama3\", base_url=\"http://127.0.0.1:11434\")  # Replace with your model and base URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n",
      "[Document(metadata={'page': 6, 'source': 'docs/rag_chunking_example.pdf'}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nPage 7'), Document(metadata={'page': 6, 'source': 'docs/rag_chunking_example.pdf'}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nPage 7'), Document(metadata={'page': 6, 'source': 'docs/rag_chunking_example.pdf'}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nPage 7'), Document(metadata={'page': 6, 'source': 'docs/rag_chunking_example.pdf'}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nPage 7')]\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"What is the document about?\"\n",
    "result = qa_chain.invoke({\"query\": query})  # Correct way to call the chain\n",
    "\n",
    "# Access the outputs\n",
    "print(result[\"result\"])  # The generated response\n",
    "print(result[\"source_documents\"])  # The retrieved documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, the document containing is \"an example document designed for testing chunking methods in Retrieval-Augmented Generation (RAG)\". It also mentions that this document contains multiple paragraphs of text that can be split into segments.\n",
      "[Document(metadata={'page': 6, 'source': 'docs/rag_chunking_example.pdf'}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nPage 7'), Document(metadata={'page': 6, 'source': 'docs/rag_chunking_example.pdf'}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nPage 7'), Document(metadata={'page': 6, 'source': 'docs/rag_chunking_example.pdf'}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nPage 7'), Document(metadata={'page': 6, 'source': 'docs/rag_chunking_example.pdf'}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nPage 7')]\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"What is the document containing?\"\n",
    "result = qa_chain.invoke({\"query\": query})  # Correct way to call the chain\n",
    "\n",
    "# Access the outputs\n",
    "print(result[\"result\"])  # The generated response\n",
    "print(result[\"source_documents\"])  # The retrieved documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The provided context only mentions \"chunking techniques\" without defining what chunking actually is. It seems to be a general term used in the text, but no further explanation or definition is given.\n",
      "[Document(metadata={'page': 13, 'source': 'docs/rag_chunking_example.pdf'}, page_content='various chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nPage 14'), Document(metadata={'page': 13, 'source': 'docs/rag_chunking_example.pdf'}, page_content='various chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nPage 14'), Document(metadata={'page': 13, 'source': 'docs/rag_chunking_example.pdf'}, page_content='various chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nPage 14'), Document(metadata={'page': 13, 'source': 'docs/rag_chunking_example.pdf'}, page_content='various chunking techniques, including overlapping windows, recursive splitting, and semantic\\nsegmentation.\\nPage 14')]\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"What is chunking?\"\n",
    "result = qa_chain.invoke({\"query\": query})  # Correct way to call the chain\n",
    "\n",
    "# Access the outputs\n",
    "print(result[\"result\"])  # The generated response\n",
    "print(result[\"source_documents\"])  # The retrieved documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: According to the context, Chunking refers to various techniques used to divide or segment text into smaller units called \"chunks\". The specific techniques mentioned are:\n",
      "\n",
      "1. Overlapping Windows\n",
      "2. Recursive Splitting\n",
      "3. Semantic Segmentation\n",
      "\n",
      "These techniques aim to group words or phrases together based on their meaning, structure, or other characteristics to create meaningful and coherent chunks of text.\n",
      "Source Documents: [{'page': 13, 'source': 'docs/rag_chunking_example.pdf'}, {'page': 13, 'source': 'docs/rag_chunking_example.pdf'}, {'page': 13, 'source': 'docs/rag_chunking_example.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "query = \"What is chunking?\"\n",
    "\n",
    "# Retrieve top-k documents\n",
    "retrieved_docs = retriever.get_relevant_documents(query=query)  # Modify your query as needed\n",
    "top_k = 5  # Set the number of top documents you want to retrieve\n",
    "retrieved_docs = retrieved_docs[:top_k]  # Retrieve only the top-k documents\n",
    "\n",
    "# Combine the content of the documents into a single context\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# Feed the combined context to the LLM\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "# Generate the response using Ollama\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "# Print the response and the source documents\n",
    "print(\"Answer:\", response)\n",
    "print(\"Source Documents:\", [doc.metadata for doc in retrieved_docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The document is not actually about anything, as it appears to be a test case for chunking methods in Retrieval-Augmented Generation (RAG) - it's just a repeating template of text designed to evaluate segmentation algorithms!\n",
      "Source Documents: [{'page': 6, 'source': 'docs/rag_chunking_example.pdf'}, {'page': 6, 'source': 'docs/rag_chunking_example.pdf'}, {'page': 6, 'source': 'docs/rag_chunking_example.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Retrieve top-k documents\n",
    "retrieved_docs = retriever.get_relevant_documents(\"What is the document about?\")  # Modify your query as needed\n",
    "top_k = 5  # Set the number of top documents you want to retrieve\n",
    "retrieved_docs = retrieved_docs[:top_k]  # Retrieve only the top-k documents\n",
    "\n",
    "# Combine the content of the documents into a single context\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# Feed the combined context to the LLM\n",
    "query = \"What is the document about?\"\n",
    "prompt = f\"Context:\\n{context} /n/nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "# Generate the response using Ollama\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "# Print the response and the source documents\n",
    "print(\"Answer:\", response)\n",
    "print(\"Source Documents:\", [doc.metadata for doc in retrieved_docs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Context with .invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Chunking refers to a technique used in natural language processing (NLP) to break down text into smaller units or \"chunks\" that are easier to process and analyze. These chunks can be phrases, sentences, clauses, or even individual words. Chunking is often used for tasks such as named entity recognition, part-of-speech tagging, sentiment analysis, and topic modeling.\n",
      "\n",
      "The three chunking techniques mentioned in the context (overlapping windows, recursive splitting, and semantic segmentation) are different approaches to chunking text into meaningful units:\n",
      "\n",
      "1. Overlapping windows: This technique involves dividing text into overlapping segments of a fixed size.\n",
      "2. Recursive splitting: This approach recursively breaks down text into smaller chunks until a desired level of granularity is reached.\n",
      "3. Semantic segmentation: This technique involves identifying meaningful units in text based on semantic or linguistic features, such as part-of-speech tags, named entities, or topic modeling.\n",
      "\n",
      "By chunking text in this way, NLP models can better understand the structure and meaning of language, allowing for more accurate processing and analysis.\n",
      "Source Documents: [{'page': 13, 'source': 'docs/rag_chunking_example.pdf'}, {'page': 13, 'source': 'docs/rag_chunking_example.pdf'}, {'page': 13, 'source': 'docs/rag_chunking_example.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "query = \"What is chunking?\"\n",
    "\n",
    "# Retrieve top-k documents using the invoke method\n",
    "retrieved_docs = retriever.invoke(query)  # Modify this to use the invoke method\n",
    "top_k = 3  # Set the number of top documents you want to retrieve\n",
    "\n",
    "# Ensure that the retrieved_docs is a list and slice to top_k\n",
    "if isinstance(retrieved_docs, dict):\n",
    "    retrieved_docs = retrieved_docs.get('documents', [])\n",
    "\n",
    "retrieved_docs = retrieved_docs[:top_k]  # Retrieve only the top-k documents\n",
    "\n",
    "# Combine the content of the documents into a single context\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# Feed the combined context to the LLM\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "# Generate the response using Ollama\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "# Print the response and the source documents\n",
    "print(\"Answer:\", response)\n",
    "print(\"Source Documents:\", [doc.metadata for doc in retrieved_docs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic RetrievalQA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I don't know.\n",
      "Source Documents: [Document(metadata={'page': 6, 'source': 'docs/rag_chunking_example.pdf'}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nPage 7'), Document(metadata={'page': 6, 'source': 'docs/rag_chunking_example.pdf'}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nPage 7'), Document(metadata={'page': 6, 'source': 'docs/rag_chunking_example.pdf'}, page_content='segmentation.\\nThis is an example document designed for testing chunking methods in Retrieval-Augmented\\nGeneration (RAG). The document contains multiple paragraphs of text that can be split into\\nPage 7')]\n"
     ]
    }
   ],
   "source": [
    "retriever.search_kwargs[\"k\"] = 3  # Set top-k for retrieval\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "query = \"What is the document about?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "# Access outputs\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "print(\"Source Documents:\", result[\"source_documents\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
